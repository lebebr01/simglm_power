---
title             : "Power Analysis by Simulation using R and simglm"
shorttitle        : "Power by Simulation"

author: 
  - name          : "Brandon LeBeau"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "brandon-lebeau@uiowa.edu"

affiliation:
  - id            : "1"
    institution   : "University of Iowa"

authornote: |
  Department of Psychological and Quantitative Foundations

abstract: |
  Power is an analysis that is commonly done prior to collecting data for a primary study. In many cases closed-form solutions are used to estimate power which make statistical assumptions to be able to perform the computations, for example to assume residuals are normally distributed. In real-world data, these statistical assumptions may not hold, therefore estimates of power when these assumptions are assumed will be different. Power by simulation is another way to compute power estimates and offers significant flexibility to the user to explore the impact statistical assumption violations may have on power. This tutorial uses the `simglm` R package to perform the power by simulation. The simglm package provides a framework to simulate data from generalized linear mixed models which are flexible to include a wide variety of models. In addition, functions to perform replications and to compute power estimate summaries are available for users to take advantage of. Two worked examples are shown, one for a two-sample t-test and another within a repeated measures or longitudinal framework. 
  
  
keywords          : "power; simulation; R; simglm"
wordcount         : "4700"

bibliography      : ["master.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : yes
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library(papaja)

```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

Statistical power is the probability that a statistical analysis is able to detect a non-zero population effect for a binary hypothesis test (add citation). In probability terms, statistical power reflects the probability of correctly rejecting the null hypothesis when it is false in the population. This description can be represented mathematically as: $power = P(reject \; H_{0} \; | \; H_{1} \; is \; true)$. The inverse of power is the probability of making a type II error or the false negative rate. 

Power analyses can take on two forms, a priori power analyses occur prior to collecting data and post hoc power analyses occur after data analysis (citation). Post hoc power analyses are controversial, can be misleading given the analysis has already been done, and can give inaccurate results when calculating power using sample data [@hoenig2001; @thomas1997]. Therefore, this paper will focus on a priori power analyses, however the process described in this paper could be applied to post hoc power analyses when appropriate. 

## Factors affecting power
There are numerous known factors that can impact power in a given analysis. The most well known include sample size, the alpha rate or false positive rate, and the magnitude of the effect size. Other factors such as statistical design, statistical analysis, missing data, or whether statistical assumptions have been met can also impact power. Often, violations of statistical assumptions will have a negative impact on power (citation), however these can and will be explore empirically below. 

# Traditional procedures for power
Power is often evaluated using closed form solutions that assume statistical assumptions hold. For example, residuals for many statistical analyses are assumed to follow a normal distribution and the residuals are also assumed to have homogeneity of variance. The general trend of assumption violation and the impact on power can be hypothesizes, however the exact magnitude that the effect will have can vary widely and could interact with other design features (citation). 

Statistical software has been developed to estimate power when these statistical assumptions have been made such as [G*Power](http://www.gpower.hhu.de/) [@faul2007], [PowerUp!](https://www.causalevaluation.org/power-analysis.html) [@powerup], or [Optimal Design](http://hlmsoft.net/od/) [@optimal]. There are also packages in statistical software programs such as [pwr](https://cran.r-project.org/package=pwr) [@pwr], [WebPower](https://cran.r-project.org/package=WebPower) [@webpower], or [stats](https://cran.r-project.org/package=STAT) which is a part of base R [@rpro] or [statsmodels](http://www.statsmodels.org/) in Python that estimate power for relatively simple statistical analyses such as t-tests, analysis of variance (ANOVA), linear regression, correlation, or general linear models. G*Power implements power for similar analyses as those found in traditional statistcal software implementations, but offers a graphical user interface (GUI) that may aid users in the power estimation. Finally, the specialized software, PowerUp! and Optimal Design are commonly used to estimate power for randomized control trials and when there are nesting effects that are common in educational or psychological research. 

Power can be estimated to explore what the probability is given a specific effect size. The minimum detectable effect size given specific sample sizes can also be estimated for power analyses (citation). The latter is often estimated when writing grant applications where the minimum detectable effect size is estimated for specified power levels.

Regardless of the framework, it needs to be articulated whether the effect size is of substantive interest and large enough to have a meaningful effect in the population. **Need to expand this**


# Power by Simulation
Power by simulation differs from the traditional approaches in the degree of flexibility the user has on the power analysis. For example, users can directly specify the generating model, analysis model, and all aspects of the generating and analysis model(s). This flexibility may better align the power analysis with the type of data that may be collected, particularly if the data is being collected in the real world where data likely do not follow all statistical assumptions. It would be useful to know the impact on power these types of violations may have. 

The following are general steps that are taken in a power by simulation example.

1. Assume population parameters, including
    - population effect size(s) of interest.
    - distribution of variable(s) and residuals.
    - variance of variable(s) and residuals.
2. Simulate data based on a statistical model.
3. Fit a statistical model to the simulated data.
4. Replicate steps 1 - 3 many times.
5. Calculate the proportion of statistical tests that reject the hypothesis.

In the power by simulation framework, data are simulated based on assumed values for the population, for example the population mean difference between two groups. Given that these values are true, data are simulated many times, replicated, and a statistical model is fitted to the simulated data. To estimate power in this framework, the number of statistical tests that reject the hypothesis compared to the number of replications

## Benefits of Power by Simulation
Power by simulation can be used for any statistical design or statistical analysis. The limiting factor is the ability of the researcher to use statistical software to follow the steps outlined above. Power by simulation can allow researchers to relax statistical assumptions that may have an impact on statistical power and more readily mirror real data conditions. If statistical assumptions do not hold in the population, the power analysis that makes these assumptions will commonly overestimate power. This can have important considerations and implications for researchers, funders, or other relavent stakeholders that are invested in the research idea.

Furthermore, as power by simulation is not limited by the software and the steps for employing a power by simulation analysis are the same regardless of the statistical design or analysis, once the process of simluation by power are well understood, the only major change across different statistical designs and analysis are the generating and fitted models. In some cases, getting estimates of population effects may be more challenging as the model complexity increases, but this can be a limiting factor of traditional power analyses as well.

The tutorial will use the following R packages

```{r packages-to-load, echo = TRUE}
library(tidyverse)
library(simglm)
library(future)
library(lme4)
```

# Two-Sample t-test Example
A two-sample t-test is a statistical technique that compares whether two means are statistically different from one another. This analysis is commonly done when two groups are measured on a single outcome and the mean of these two groups are compared to one another. As an example, the males and females may be compared on their average reading score or average depression score after a treatment. 

Power for a two-sample design using the t-test can be done with the following code for a single effect size, a standardized mean difference of 0.15. This example generates power for sample sizes ranging from 4 up to 1000 increasing by intervals of 2. This will generate power values for 499 sample sizes. 


```{r power-t-test, echo = TRUE}
n <- seq(4, 1000, 2)
power <- sapply(seq_along(n), function(i) 
  power.t.test(n = n[i], delta = .15, sd = 1, type = 'two.sample')$power)
```

The power for the first iteration can be extracted directly using the code `power[1]`, which returns a value of `r round(power[1], 3)`. There are numerous power values however and figures are a great way to show all of these values in a relatively succict way. The code below creates a data frame that includes the sample size and subsequent power values then uses the ggplot2 [@ggplot2] package to visualize the power estimates. A horizontal line is drawn at a power level of 0.80, a common standard, and a vertical line is drawn when the curve hits this threshold which happens to occur at a sample size of 700. This sample size does not represent the total sample size, rather it represents the sample size for each group. The `power.t.test` function also assumes balanced groups, homogeneity of variance, normally distributed residuals, independent observations to obtain the power estimates.

```{r power-figure, echo = TRUE, fig.cap = "Power estimates for an effect size of 0.15 in the two-sample design."}
power_df <- data.frame(
  n = n,
  power = power
)

ggplot(power_df, aes(x = n, y = power)) + 
  geom_line(size = 2) + 
  geom_hline(yintercept = 0.8, linetype = 2, color = 'gray30') + 
  geom_vline(xintercept = 700, linetype = 2, color = 'gray30') +
  scale_x_continuous("Sample Size", breaks = seq(0, 1000, 200)) + 
  scale_y_continuous("Power", breaks = seq(0, 1, .2)) +
  theme_bw(base_size = 14)
```

## Power Curves
It is common for the effect size of interest to not be completely certain a priori. In these cases, descriptive power analyses are often conducted that vary the effect size as well as the sample size to explore the power estimates for different effect sizes across a variety of sample sizes. 

A similar structure can be done to add the power curves for different effect sizes; the primary differences being the addition of different effect sizes in addition to the different sample size conditions. These conditions are created using the `expand.grid` function which creates a row for each unique value of the variables specified. These conditions are then passed to the `power.t.test` function one at a time to create power estimates for each of the data conditions. 

```{r power-curve, echo = TRUE}
effect_sizes <- c(.10, .15, .25)
conditions <- expand.grid(n = n, effect_sizes = effect_sizes)

power_curve <- sapply(seq_len(nrow(conditions)), function(i) 
  power.t.test(n = conditions[i, 'n'], 
               delta = conditions[i, 'effect_sizes'], 
               sd = 1, type = 'two.sample')$power)
```

A similar process is used to create a data frame with the conditions and power estimates to be visualized. Figure \@ref(fig:vis-power-curve) shows the three power curves based on the three effects sizes of 0.10, 0.15, and 0.25. You'll notice that for the effect size of 0.10, estimated power never reaches the desired threshold of 0.80 indicating that larger sample sizes for each group would be needed. In addition, power estimates for an effect size of 0.25 reaches the threshold of 0.80 with around 250 individuals in each group. Having sample sizes much larger than that results in much smaller changes to the estimated power.

```{r vis-power-curve, echo = TRUE, fig.cap = "Three power curves representing power estimates across three different effect sizes and various sample sizes."}
power_curve_df <- bind_cols(
  conditions, 
  power = power_curve
)

ggplot(power_curve_df, aes(x = n, y = power)) + 
  geom_line(aes(color = factor(effect_sizes)), size = 2) + 
  geom_hline(yintercept = 0.8, linetype = 2, color = 'gray30') + 
  scale_x_continuous("Sample Size", breaks = seq(0, 1000, 200)) + 
  scale_y_continuous("Power", breaks = seq(0, 1, .2)) +
  scale_color_grey("Effect Size") +
  theme_bw(base_size = 14)
```




# Two sample Data simulation with `simglm`
The same two sample power analysis can be conducted by simulation with the `simglm` R package [@simglm]. This package simulates data based on general(-ized) linear (mixed) models and can replicate the simulation procedure to perform a power analysis. In order to use the `simglm` package, the two-sample t-test described above needs to be reformulated into an equivalent form using the general linear model or more specifically a linear regression model. 

The linear regression model would look like the following:
\begin{equation} 
Y_{i} = \beta_{0} + \beta_{1} group_{i} + \epsilon_{i} 
(\#eq:twosamp)
\end{equation}

\noindent where $Y_{i}$ is the outcome of interest (i.e. reading or depression scores reference above) for individual $i$, $group_{i}$ is a fixed indicator variable that would have a score of 1 if an individual belonged to group 1 otherwise would have a score of 0, and $\epsilon_{i}$ would represent random sampling error. With the linear regression model set up this way, $\beta_{0}$ would represent the intercept and would be interpreted as the mean of the group coded as 0 in the $group_{i}$ variable. The second coefficient, $\beta_{1}$, would be the variable of interest and represent the mean difference between the two groups. The mean of the second group could be found by taking $\beta_{0} + \beta_{1}$. The primary benefit of approaching the two-sample t-test in this fashion is the model specification flexiblity. For example, if statistical controls or interactions would want to be explored it can be done in the linear modeling framework much easier. 

The following code is an example of simulating data using the `simglm` package. The code consists of two parts, the first part specifies the parameters for the simulation in a named list, called `simulation_arguments` below. For the two-sample linear regression model shown above, the following named elements need to be included, the model formula (which mimics the linear regression formula depicted above without the error), the fixed component representing the group variable (depicted as sex below), the sample size (i.e. how many individuals), the size of random error variance, and the regression weights. The regression weights (named `reg_weights` below) represent the values for $\beta_{0}$ and $\beta_{1}$ discussed above in Equation \@ref(eq:twosamp). Exploring the code below, this would mean that the mean for group 1 would be assumed to be 0 and the mean difference between the two groups is 0.15 (equivalently here the mean of group 2 would be $0 + 0.15 = 0.15$). Since the error variance is specified to equal 1, the data generated mimics a standardized example and the interpretation of the second regression weight (i.e. $\beta_{1} = 0.15$) would be similar to the effect size, cohen's d (citation). 

Finally, the fixed argument in some cases will be the most difficult argument to specify and often requires the most code. In this example, only the sex variable needs to be defined to specify how this should be generated. The code for the fixed portion identifies that this variable should be a factor variable (i.e. a variable that has a limited number of categories and these categories do not have any natural order to them), using the `var_type` argument, and the levels of the sex variable (i.e. male and female) are specified. This ensures that the data labels are created directly rather than just numeric values and can be helpful to ensure the creation of the two groups. Internally during the simulation process this variable with character labels would be turned into a dichotomous indicator variable that is defined above in Equation \@ref(eq:twosamp). 

```{r simglm-two-samp-simargs, echo = TRUE}
simulation_arguments <- list(
  formula = y ~ 1 + sex,
  fixed = list(sex = list(var_type = 'factor', 
                            levels = c('male', 'female'))),
  sample_size = 20,
  error = list(variance = 1),
  reg_weights = c(0, .15)
)
```

A series of commands are used for the generation of the simulated data, `simulate_fixed`, `simulate_error`, and `generate_response`. In general, these functions take the simulation arguments defined in the previous code chunk as the primary argument and the `%>%` pipe operator is used to chain the commands together which passes the result from the previous step to the first argument of subsequent function calls. For the simglm package, the first argument of the functions is always the data. The `simulate_fixed` function generates the fixed portion of the model representing the intercept and sex variables respectively (see Equation \@ref(eq:twosamp)). The `simulate_error` function generates the random sampling error and `generate_response` takes the results from the first two functions to generate the outcome.

```{r simulate-data, eval = FALSE, echo = TRUE, }
simulate_fixed(data = NULL, simulation_arguments) %>%
  simulate_error(simulation_arguments) %>%
  generate_response(simulation_arguments)
```

Table \@ref(tab:simulate-data-out) shows the first six rows of the simulated data. The output shows the values for the sex variable which would represent the variable of interest and the outcome variable here is represented as the last column labeled y. The random error values are also given in the error column. It is useful to explore one simulated data set prior to replicating the simulation to ensure that it was simulated as specified. In addition, the output in Table \@ref(tab:simulat-data-out) shows the the mean difference of 0.15 would indicate that males are 0.15 standard units higher than females. If the opposite was desired, the parameter in the `reg_weights` argument could be made negative (i.e. -0.15) which would indicate males have a smaller mean. Note, the order of the sex variable is due to how the factor variables are generated, where the different levels are generated in alphabetical order. Therefore, females are designated as the first group and males are the second group, which translates into this being specified as the indicator variable when creating the design matrix for the fixed effects.

```{r simulate-data-out, caption = "The output from the simulated data"}
simulate_fixed(data = NULL, simulation_arguments) %>%
  simulate_error(simulation_arguments) %>%
  generate_response(simulation_arguments) %>%
  head() %>%
  knitr::kable(format = "latex", digits = 2, booktabs = TRUE,
        caption = "The output from the simulated data generated with simglm")
```

## Using `simglm` to generate power
Very few adjustments are need to generate power using the `simglm` package once the simulation code for one replication is successful. The number of replications need to be specified, a specification of what kind of model should be fitted, and whether to extract the model coefficients. The number of replications indicate how many times the data will be simulated and subsequently how many times model estimates will be obtained. In general, larger is better and will increase precision for the power estimates that are obtained. The number of replications can be thought of as similar to sample size in primary studies. In the example below, the number of replications is set to 1000 which is a minimum value that I would recommend and was used here as a trade-off between time to run the power analysis and precision of the power estimates. In general, the more replications that are run, the longer the power analysis will take. 

The model to be fitted is specified with the names `model_fit` argument. Only the `model_function` needs to be specified within the `model_fit` list to indicate which R model function should be used. The additional argument of model formula is also shown below as this gives the user additional flexibility and would allow the user to explore the impact of model misspecification (i.e. not including a model variable that is present in the population or data generating model). If the model formula is omitted from the `model_fit` argument, the same model formula as the simulated model is specified. Finally, the `extract_coefficients` is commonly specified as TRUE to indicate that the estimated coefficients from the fitting of models from `model_fit` will be returned. If the `extract_coefficients` argument is missing from the simulation arguments, then the fitted model object is returned which would need to be processed to extract the desired information.

```{r simglm-two-samp-power, echo = TRUE}
plan(multiprocess)

simulation_arguments <- list(
  formula = y ~ 1 + sex,
  fixed = list(sex = list(var_type = 'factor', 
                            levels = c('male', 'female'))),
  sample_size = 20,
  error = list(variance = 1),
  reg_weights = c(0, .15),
  replications = 1000,
  model_fit = list(formula = y ~ 1 + sex, 
                   model_function = 'lm'),
  extract_coefficients = TRUE
)

replicate_sim <- replicate_simulation(simulation_arguments)
```

Finally, one last function is needed to compute power and other useful summary statistics based on the analysis. The `compute_statistics` function is used to compute these summary statistics from the replicated simulation results. The primary arguments for this function include the simulation arguments and which statistics should be returned. In the example below, power and precision are set to TRUE to return those statistics and type I error statistics are omitted as power is of most interest here. For simulation studies to explore methodological impacts of assumption violations, type I error may be of interest.

```{r compute-stats, echo = TRUE, eval = FALSE}
replicate_sim %>%
  compute_statistics(simulation_arguments, power = TRUE,
                     type_1_error = FALSE, precision = TRUE)
```

Table \@ref(tab:compute-stat-tab) shows the output from the power analysis with simglm. The output shows the information for power and precision. In the output, if power estimates are of most concern, the columns "Avg Est" and "Power" are most relavent which provide estimates for the average estimates and the number of binary hypotheses correctly rejected across all replications. The additional information is more for reference, but contain information on the average test statistic, parameter estimate standard deviation, average standard error, precision (estimated standard deviation / average standard error), and number of replications. In the example shown above with only a sample size of 20 individuals, the estimated power was 0.08 meaning that the null hypothesis would only be correctly rejected 8% of the time. 

```{r compute-stat-tab}
replicate_sim %>%
  compute_statistics(simulation_arguments, power = TRUE,
                     type_1_error = FALSE, precision = TRUE) %>%
  knitr::kable(digits = 2, format = 'latex', booktabs = TRUE,
        caption = "Statistical power and precision output obtained from the simglm package.", 
        col.names = c('Term', 'Avg Est', 'Power', 'Avg TS', 'Crit Value', 'Est SD', 'Avg SE', 
        'Precision', 'Repl'))
```


## Varying arguments with `simglm`
The example with `simglm` above only considered power for a single sample size, but a power-curve was shown for the closed form solution above that reflected various sample sizes and various effect sizes simultaneously. This can be achieved in a single analysis by using the `vary_arguments` argument specified in the simulation arguments. As shown in the code below, the `sample_size` argument is removed from the main simulation arguments and it is instead specified within the `vary_arguments` argument. The sample size is generated to indicate that the sample size should be evaluated for the sequence starting at 20, ending at 2000, and incrementing by 20. When specifying arguments to vary, the default behavior is to do a factorial design meaning that all combinations are considered and replicated as part of the power analysis. More specifically for this example, this would mean that sample sizes of 20, 40, 60, up to 2000 will be run and replicated 1000 times.

Finally, an optional argument named, `power`, is shown that provides further user customization to how power is calculated. For example, the power arguments defined below indicate that a t-distribution critical value with an alpha value of 0.05 and the default behavior is to assume a two-tailed hypothesis test. By default, the degrees of freedom for the t-distribution is computed by taking the total sample size minus one. This means that once sample size gets above about 100, the t-distribution critical value and the z-distribution critical value would be very similar. If a fixed degrees of freedom value would want to be used, this can be specified within the `opts` argument passed to the power element of the simulation arguments.

```{r power-vary-arguments, cache = TRUE, echo = TRUE}
simulation_arguments <- list(
  formula = y ~ 1 + sex, 
  fixed = list(sex = list(var_type = 'factor', 
                            levels = c('male', 'female'))),
  error = list(variance = 1),
  reg_weights = c(0, .15),
  replications = 1000,
  model_fit = list(formula = y ~ 1 + sex,
                   model_function = 'lm'),
  power = list(
    dist = 'qt',
    alpha = .05
  ),
  extract_coefficients = TRUE,
  vary_arguments = list(
    sample_size = seq(20, 2000, 20) 
  )
)

model_results <- replicate_simulation(simulation_arguments) 
```

The `compute_statistics` function is again used to generate summary statistics. Internally, the arguments that are varied are used as grouping variables when computing these summary statistics, therefore power values will be computed for all unique values for arguments that were varied. In addition, some post-processing is done to turn the sample size values from a factor variable to numeric for easier use later on and only the power values for the sex variable are returned as this is the power effect of interest.

```{r power-results, echo = TRUE}
power_results <- model_results %>%
  compute_statistics(simulation_arguments, power = TRUE,
                     type_1_error = FALSE, precision = TRUE)

power_results <- power_results %>%
  ungroup() %>%
  mutate(sample_size = as.numeric(as.character(sample_size))) %>%
  arrange(sample_size) %>%
  filter(term == 'sex')
```

The tabular results could be explored by viewing the `power_results` object directly, however this would be more difficult to interpret quickly due to the many sample sizes generated. Therefore, the result can be explored visually to generate a power curve across the sample sizes for this single effect size. The results mimic was would found in Figure \@ref(fig:power-figure) where sample sizes for each group was approximately 700. 

```{r simglm-power-curve, message = FALSE, echo = TRUE, fig.cap = "A single power curve for varying sample size conditions with a single effect size using simglm."}
ggplot(power_results, aes(x = sample_size, y = power)) + 
  geom_point(size = 1.5, color = 'gray40') + 
  geom_hline(yintercept = 0.8, linetype = 2, color = 'gray30') + 
  geom_smooth(linetype = 1, size = 1, se = FALSE) +
  geom_vline(xintercept = min(filter(power_results, power >= 0.8)$power), 
             linetype = 2, color = 'gray30') +
  scale_x_continuous("Sample Size", breaks = seq(0, 2000, 200)) + 
  scale_y_continuous("Power", breaks = seq(0, 1, .2)) +
  theme_bw(base_size = 14)
```


## Visualizing estimates and p-values
Exploring the estimates and the p-values can be informative over and above any power curves generated. The model results are stored in the object, `model_results`, as a list for each replicated condition. This means that for each sample size, there are 1000 estimates for the sex effect and the associated p-values based on these. 

The `model_results` object is a list across sample size conditions, therefore this is first combined into a dataframe and then plotted with ggplot2. The resulting figure shows the increased precision as sample size increases. More specifically, the standard error is decreasing, but on average the parameter estimate is centered around 0.15. 

```{r estimate-figure, echo = TRUE, fig.cap = "Distribution of parameter estimates for the sex effect by sample size."}
model_results_df <- bind_rows(model_results) %>%
  filter(term == 'sex') %>%
  mutate(sample_size = as.numeric(as.character(sample_size)))

ggplot(model_results_df, aes(x = sample_size, y = estimate)) + 
  geom_boxplot(aes(group = sample_size), outlier.alpha = 0.01) +
  scale_x_continuous("Sample Size", breaks = seq(0, 2000, 200)) +
  theme_bw(base_size = 14) +
  geom_hline(yintercept = .15, linetype = 2, size = 2, color = 'gray20') +
  coord_flip()
```

A similar figure can be created showing the distribution of p-values, which gives a similar picture. There is much more variation when the sample sizes are small compared to larger sample sizes which reflects the smaller standard errors. 

```{r p-value-figure, echo = TRUE}
ggplot(model_results_df, aes(x = sample_size, y = p.value)) + 
  geom_boxplot(aes(group = sample_size), outlier.alpha = 0.01) +
  scale_x_continuous("Sample Size", breaks = seq(0, 2000, 200)) +
  ylab("p-value") +
  theme_bw(base_size = 14) +
  geom_hline(yintercept = .05, linetype = 2, size = 2, color = 'gray20') +
  coord_flip()
```

## Add Heterogeneity
Heterogeneity is a common phenomenon that can impact power and can occur when there are population variance differences across the two groups. In the above example, this would mean that there would be variance differences across males and females, e.g. perhaps males are more variable in the outcome. The `power.t.test` function from before does not assume heterogeneity, however this is possible to incorporate when doing power from a simulation framework. Adding these conditions can help to mimic real world conditions and also maybe provide a better estimate of power.

To simulate heterogeneity, an additional argument to the simulation arguments is added called `heterogeneity`. This argument specifies the group to which the heterogeneity is applied, in this case the group/sex variable and the specific variances for the groups of that variable. In this example, there are only two groups therefore only two variances need to be supplied. The variance for the second group is set to be about 8 times that of the first group, a ratio that exceeds a common heuristic of 3 times the variance ratio indicating a violation of the statistical assumption homogeneity of variance (citation). 

```{r heterogeneity, cache = TRUE, echo = TRUE}
simulation_arguments <- list(
  formula = y ~ 1 + group,
  fixed = list(group = list(var_type = 'factor', 
                            levels = c('male', 'female'))),
  error = list(variance = 1),
  heterogeneity = list(variable = 'group',
                       variance = c(1, 8)),
  reg_weights = c(0, .15),
  replications = 1000,
  model_fit = list(formula = y ~ 1 + group, 
                   model_function = 'lm'),
  power = list(
    dist = 'qnorm',
    alpha = .05
  ),
  extract_coefficients = TRUE,
  vary_arguments = list(
    sample_size = seq(20, 2000, 20) 
  )
)

model_results_h <- replicate_simulation(simulation_arguments)
```

These results are then processed similarly to above and combined with the power results that assumed a constant variance across the two groups. Figure \@ref(fig:simglm-power-curve-h) shows the power estimates contrasting results with and without heterogeneity. The effect of heterogeneity is quite stark with significantly reduced power estimates when there is heterogeneity. Smaller variance ratios could be considered to explore smaller ratios and the impact on power estimates. 

```{r heterogeneity-data-manip, echo = TRUE}
power_results_h <- model_results_h %>%
  compute_statistics(simulation_arguments, power = TRUE,
                     type_1_error = FALSE, precision = TRUE) %>%
  ungroup() %>%
  mutate(sample_size = as.numeric(as.character(sample_size)),
         heterogeneity = TRUE) %>%
  arrange(sample_size) %>%
  filter(term == 'group')

power_results <- power_results %>%
  mutate(heterogeneity = FALSE)

power_results_combined <- bind_rows(power_results, 
                                    power_results_h)
```

```{r simglm-power-curve-h, message = FALSE, echo = TRUE, fig.cap = "Power estimates for a single effect size (0.15) by differing sample sizes and whether there is variance heterogeneity."}
ggplot(power_results_combined, aes(x = sample_size, y = power, 
                          group = heterogeneity)) + 
  geom_point(aes(shape = heterogeneity), size = 1.5, color = 'gray40') + 
  geom_hline(yintercept = 0.8, linetype = 2, color = 'gray30') + 
  geom_smooth(aes(linetype = heterogeneity), size = 1, se = FALSE) +
  scale_x_continuous("Sample Size", breaks = seq(0, 2000, 200)) + 
  scale_y_continuous("Power", breaks = seq(0, 1, .2)) +
  theme_bw(base_size = 14)
```


# Repeated Measures Example
For more complicated designs, closed form solutions are not always possible as ways to estimate statistical power or they make strong assumptions on the data that may not be reasonable given the data to be collected. The data simulation for the repeated measures example adds a hierarchical data structure where repeated measures are nested within individuals. These type of data add a correlation structure that needs to be modeled appropriately for valid inferences. Modern models to handle this dependency include the linear mixed model (LMM) [@fitzmaurice2012], sometimes known as the hierarchical linear model (HLM) [@raudenbush2002] or the multilevel model [@goldstein2011]. 

This framework is an extension of the linear model considered above in the two group model. The extension includes the addition of random effects which represent subject specific deviations from the average trejectories. These random effects are what account for the dependency due to repeated measurements and nested data structure. In addition to the addition of random effects, two sample sizes need to be given, one representing the number of measurement occasions for each individual and another representing how many individuals to generate. Therefore, in the balanced case (i.e. same number of measurement occasions for each individual), the total sample size (i.e. number of rows in the data) will be the number of measurement occasions times the number of individuals. This is a good initial check to ensure that the number of records are correct.

The code below also introduces a new argument to the fixed and randomeffect arguments, called `var_level` which indicates at which level the variable belongs to. The default value is level 1 which would represent the repeated measures level and level 2 would represent individuals. In this case, the sex variable and the random effects need to be specified at level 2 or the individual level (i.e. these values should be constant for an individual rather than vary within an individual). This is done for these variables by setting `var_level = 2`. Similarly, the sample sizes are specified for level 1 and level 2 separately in the `sample_size` argument with `level1 = 10` and `level2 = 500` respectively. 

```{r sim-longitudinal, echo = TRUE}
sim_arguments <- list(
  formula = y ~ 1 + time + sex + time:sex + (1 + time | individual),
  reg_weights = c(4, 0.4, 0.3, 0.20),
  error = list(variance = 1),
  fixed = list(time = list(var_type = 'time'),
               sex = list(var_type = 'factor', levels = c('male', 'female'),
                          var_level = 2)),
  randomeffect = list(int_individual = list(variance = 1, var_level = 2),
                      time_individual = list(variance = 0.5, var_level = 2)),
  sample_size = list(level1 = 10, level2 = 500)
)

longitudinal_data <- sim_arguments %>%
  simulate_fixed(data = NULL, .) %>%
  simulate_randomeffect(sim_arguments) %>%
  simulate_error(sim_arguments) %>%
  generate_response(sim_arguments)
```

The data structure for the simulated data is not shown directly as it would look very similar to that shown in Table \@ref(tab:simulate-data-out).

## Power for Repeated Measures
Estimating power for repeated measures is very similar to that found when doing power for the two group linear model. The addition of number of replications, model fitting instructions, and whether to extract model coefficients are specified with the `replications`, `model_fit`, and `extract_coefficients` simulation arguments respectively. 

```{r rm-power, cache = TRUE, warning = FALSE, message = FALSE, echo = TRUE}
sim_arguments <- list(
  formula = y ~ 1 + time + sex + time:sex + (1 + time | individual),
  reg_weights = c(4, 0.4, 0.3, 0.20),
  error = list(variance = 1),
  fixed = list(time = list(var_type = 'time'),
               sex = list(var_type = 'factor', levels = c('male', 'female'),
                          var_level = 2)),
  randomeffect = list(int_individual = list(variance = 1, var_level = 2),
                      time_individual = list(variance = 0.5, var_level = 2)),
  sample_size = list(level1 = 10, level2 = 500),
  replications = 1000,
  model_fit = list(formula = y ~ 1 + time + sex + time:sex + 
                     (1 + time | individual), 
                   model_function = 'lmer'),
  extract_coefficients = TRUE
)

long_models <- replicate_simulation(sim_arguments)
```

The `compute_statistics` function is again used to summarize the power estimates by the terms in the model. Power and precision summary statistics are returned directly and summarized in Table \@ref(tab:long-power). Power estimates reveal that power is above 0.8 for all variables in the model.

```{r long-compute, echo = TRUE, eval = FALSE}
long_models %>%
  compute_statistics(sim_arguments, power = TRUE,
                     type_1_error = FALSE, precision = TRUE)
```

```{r long-power}
long_models %>%
  compute_statistics(sim_arguments, power = TRUE,
                     type_1_error = FALSE, precision = TRUE) %>%
  knitr::kable(digits = 2, format = 'latex', booktabs = TRUE,
        caption = "Statistical power and precision output obtained from the repeated measures example.", 
        col.names = c('Term', 'Avg Est', 'Power', 'Avg TS', 'Crit Value', 'Est SD', 'Avg SE', 
        'Precision', 'Repl'))
```



### Varying simulation conditions
Sample size is now varied to explore the impact that different level 1 and level 2 sample sizes have on power estimates. The `vary_arguments` simulation argument is used to control this behavior where the level 1 sample size was specified to either be 5 or 8 measurement occasions and the level 2 sample size was varied to be either 50, 150, or 250 individuals. 

```{r vary-conditions, cache = TRUE, warning = FALSE, message = FALSE, echo = TRUE}
sim_arguments <- list(
  formula = y ~ 1 + time + sex + time:sex + (1 + time | individual),
  reg_weights = c(4, 0.4, 0.3, 0.20),
  error = list(variance = 1),
  fixed = list(time = list(var_type = 'time'),
               sex = list(var_type = 'factor', levels = c('male', 'female'),
                          var_level = 2)),
  randomeffect = list(int_individual = list(variance = 1, var_level = 2),
                      time_individual = list(variance = 0.5, var_level = 2)),
  replications = 1000,
  model_fit = list(formula = y ~ 1 + time + sex + time:sex + 
                     (1 + time | individual), 
                   model_function = 'lmer'),
  extract_coefficients = TRUE,
  vary_arguments = list(
    sample_size = list(list(level1 = 5, level2 = 50),
                       list(level1 = 5, level2 = 150),
                       list(level1 = 5, level2 = 250),
                       list(level1 = 8, level2 = 50),
                       list(level1 = 8, level2 = 150),
                       list(level1 = 8, level2 = 250))
  )
)

long_model_nomiss <- replicate_simulation(sim_arguments) 
```

The summary statistics from the different sample size conditions are summarized and save in the object, `long_power_nomiss`. 

```{r long-model-power, echo = TRUE}
long_power_nomiss <- long_model_nomiss %>%
  compute_statistics(sim_arguments, power = TRUE,
                     type_1_error = FALSE, precision = TRUE)
```

In addition to just sample size, missing data can also be added to the design with the simulation argument, `missing_data`. Dropout missing data can be common in longitudinal designs where an individual no longer participates in the study. For example, an individual may come for the first two measurement occasions, but the subsequent measurement occasions for that individual are missing. Arguments within the `missing_data` include the new outcome variable to include in the data object that includes the missing data, in the example below this is named `new_outcome`, the proportion of missing data with `miss_prop`, the cluster variable called `clust_var`, and the type of missing data mechanism with `type`. To call dropout missing data `type = "dropout"`. The simglm documentation contains information on additional missing data mechanisms that are currently supported. The new outcome variable needs to be specifyied in the `model_fit` simulation argument to ensure the outcome with missing data is used for the model fitting. More specifically, the outcome in the model fitting step should be `y_miss` instead of `y` to ensure that the missing data are dropped from the model. 

```{r long-model-miss, cache = TRUE, warning = FALSE, message = FALSE, echo = TRUE}
sim_arguments <- list(
  formula = y ~ 1 + time + sex + time:sex + (1 + time | individual),
  reg_weights = c(4, 0.4, 0.3, 0.20),
  error = list(variance = 1),
  fixed = list(time = list(var_type = 'time'),
               sex = list(var_type = 'factor', levels = c('male', 'female'),
                          var_level = 2)),
  randomeffect = list(int_individual = list(variance = 1, var_level = 2),
                      time_individual = list(variance = 0.5, var_level = 2)),
  replications = 1000,
  model_fit = list(formula = y_miss ~ 1 + time + sex + time:sex + 
                     (1 + time | individual), 
                   model_function = 'lmer'),
  missing_data = list(new_outcome = 'y_miss', miss_prop = .20,
                      clust_var = 'individual', type = 'dropout'),
  extract_coefficients = TRUE,
  vary_arguments = list(
    sample_size = list(list(level1 = 5, level2 = 50),
                       list(level1 = 5, level2 = 150),
                       list(level1 = 5, level2 = 250),
                       list(level1 = 8, level2 = 50),
                       list(level1 = 8, level2 = 150),
                       list(level1 = 8, level2 = 250))
  )
)

long_model_miss <- replicate_simulation(sim_arguments) 
```

Upon simulation and model fitting, the power and precision summary statistics are calculated and the power estimates are combined for the missing and complete data conditions. Some data processing is performed to make the sample size data labels easier to read in the subsequent figure. 

```{r long-power-miss, echo = TRUE}
long_power_miss <- long_model_miss %>%
  compute_statistics(sim_arguments, power = TRUE,
                     type_1_error = FALSE, precision = TRUE)

long_power_nomiss_h <- long_power_nomiss %>%
  ungroup() %>%
  mutate(sample_size = gsub("^list\\(|\\)$", "", as.character(sample_size)),
         missing = FALSE)

long_power_miss_h <- long_power_miss %>%
  ungroup() %>%
  mutate(sample_size = gsub("^list\\(|\\)$", "", as.character(sample_size)), 
         missing = TRUE)

long_power_combined <- bind_rows(long_power_nomiss_h, 
                                    long_power_miss_h)
```

Figures \@ref(fig:long-power-curve-h) and \@ref(fig:long-power-curve-ts) show the power estimates by various sample size conditions and whether there was missing data for the sex and time by sex interaction effects respectively. The results show that the level 2 sample size has a greater impact on power compared to changes in the level 1 sample size. In addition, the dropout missing data has greater impacts on the time by sex interaction effect compared to the sex effect. This is not surprising as the sex effect is exploring changes in the intercept based on whether the individual is male or female. In general across all of the conditions, power does not reach the often desired level of 0.80, but is closer for the time by sex interaction with the largest sample sizes. 

```{r long-power-curve-h, message = FALSE, echo = TRUE, fig.cap = "Power estimates for different sample sizes and whether missing data is included for the sex effect."}
long_power_sex <- filter(long_power_combined, term == 'sex')

ggplot(long_power_sex, aes(x = reorder(sample_size, desc(power)), y = power, 
                          group = missing)) + 
  geom_point(aes(shape = missing), size = 1.5, color = 'gray40') + 
  geom_line(aes(linetype = missing), size = 1) +
  geom_hline(yintercept = 0.8, linetype = 2, color = 'gray30') + 
  xlab("Sample Size") + 
  scale_y_continuous("Power", breaks = seq(0, 1, .2), limits = c(0, 1)) +
  theme_bw(base_size = 14) + 
  coord_flip()
```

```{r long-power-curve-ts, message = FALSE, echo = TRUE, fig.cap = "Power estimates for different sample sizes and whether missing data is included for the time by sex interaction."}
long_power_timesex <- filter(long_power_combined, term == 'time:sex')

ggplot(long_power_timesex, aes(x = reorder(sample_size, desc(power)), y = power, 
                          group = missing)) + 
  geom_point(aes(shape = missing), size = 1.5, color = 'gray40') + 
  geom_line(aes(linetype = missing), size = 1) +
  geom_hline(yintercept = 0.8, linetype = 2, color = 'gray30') + 
  #geom_smooth(aes(linetype = missing), size = 1, se = FALSE) +
  #geom_vline(xintercept = 1488, linetype = 2, color = 'gray30') +
  xlab("Sample Size") + 
  scale_y_continuous("Power", breaks = seq(0, 1, .2), limits = c(0, 1)) +
  theme_bw(base_size = 14) + 
  coord_flip()
```

# Summary
This paper works through power examples in the traditional framework first, then explores how to generate power estimates through simulation for two common research designs, the two group linear model and repeated measures. When statistical assumptions held, the power estimates were similar between the traditional and simulation frameworks (see \@ref(fig:power-figure) \@ref(fig:simglm-power-curve)). This is not surprising given the assumptions that closed form power solutions make to estimate power. 

The benefits of estimating power through a simulation framework occurs through the flexibility that the procedure has and the ability to mimic real world data conditions. In addition, there are models for which closed form solutions to estimate power are not as readily available, for example generalized linear models that have outcomes that are non-normally distributed and/or not continuous are more difficult to obtain closed form solutions for power. 

In this tutorial, the `simglm` R package was used for power by simulation. The `simglm` package provides a framework to simulate data from a variety of generalized linear models including models that have nested or cross-classified data structures. The package also provides users ways to replicate the simulation design, vary simulation arguments, and estimate power with wrapper functions that do not require users to have to program the replications or data summarization. Users are directed to the `simglm` documentation, vignettes, and GitHub page (https://github.com/lebebr01/simglm) for additional examples and package options. The additional options include the ability to simulate models with binary or count outcomes, add additional missing data mechanisms, add additional levels of nested or cross-classifated data structures, and generate different types of variables. 


\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
